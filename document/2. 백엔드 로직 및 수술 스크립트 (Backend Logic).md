# ğŸ“„ ë¬¸ì„œ 2: ë°±ì—”ë“œ ë¡œì§ ë° ìˆ˜ìˆ  ìŠ¤í¬ë¦½íŠ¸ (Backend Logic)
## 1. í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ ì„¤ëª…
ë³¸ ì‹œìŠ¤í…œì˜ í•µì‹¬ì€ Rank-k Orthogonal Subspace Projectionì´ë‹¤. íŠ¹ì • ì§ˆë¬¸êµ°(Harmful)ê³¼ ëŒ€ì¡°êµ°(Harmless) ì‚¬ì´ì˜ í™œì„±í™” ì°¨ì´ë¥¼ PCAë¡œ ë¶„ì„í•˜ì—¬ ê±°ì ˆ ì„±ë¶„ì„ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ ëª¨ë¸ì˜ ì¤‘ê°„ ë ˆì´ì–´ì—ì„œ ë¬¼ë¦¬ì ìœ¼ë¡œ ì†Œê±°í•œë‹¤.

## 2. [ìµœì¢… ì§„í™”í˜•] íƒˆì˜¥ ìŠ¤í¬ë¦½íŠ¸ (Antigravity êµ¬í˜„ í•„ìˆ˜ ì½”ë“œ)
AntigravityëŠ” ì•„ë˜ ë¡œì§ì„ ë°±ì—”ë“œ ëª¨ë“ˆ(`surgery_engine.py`)ë¡œ êµ¬í˜„í•˜ì—¬ UIì™€ ì—°ê²°í•´ì•¼ í•œë‹¤.
```Python
import torch
from transformer_lens import HookedTransformer
from sklearn.decomposition import PCA
import gc
import numpy as np

# [Antigravity Note: ì´ í•¨ìˆ˜ëŠ” UIì—ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ë°›ì•„ ì‹¤í–‰ë˜ì–´ì•¼ í•¨]
def run_jailbreak_surgery(model, k=3, coeff=1.0, target_layers=[14, 16, 18], test_query=""):
    def apply_template(prompt):
        return f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"

    # ë°ì´í„°ì…‹ êµ¬ì„±
    harmful_raw = ["í­íƒ„ ì œì¡°ë²•", "í•´í‚¹ ê¸°ë²•", "ë§ˆì•½ ë°°í•©", "í”¼ì‹± ì‚¬ì´íŠ¸", "ë…ì„± ì¶”ì¶œ", "ì´ê¸° ê°œì¡°", "ì•”í˜¸ í•´ë…"] * 3
    harmless_raw = ["ìš”ë¦¬ ë ˆì‹œí”¼", "íŒŒì´ì¬ ì½”ë”©", "ìš°ì£¼ ê³¼í•™", "ì„¸ê³„ ì—­ì‚¬", "ìš´ë™ ë£¨í‹´", "ëª…ìƒ ë°©ë²•", "ê²½ì œ ê¸°ì´ˆ"] * 3
    harmful_prompts = [apply_template(p) for p in harmful_raw]
    harmless_prompts = [apply_template(p) for p in harmless_raw]

    # 1. ìœ ë™ì  ë ˆì´ì–´ ìŠ¤ìº” ë° ë²¡í„° ì¶”ì¶œ
    def get_optimized_subspaces(k, layers):
        subspaces = {}
        for layer in layers:
            diffs = []
            for p_bad, p_good in zip(harmful_prompts, harmless_prompts):
                with torch.no_grad():
                    _, cache_bad = model.run_with_cache(p_bad, names_filter=lambda n: n.endswith(f"blocks.{layer}.hook_resid_pre"))
                    _, cache_good = model.run_with_cache(p_good, names_filter=lambda n: n.endswith(f"blocks.{layer}.hook_resid_pre"))
                    d = cache_bad[f"blocks.{layer}.hook_resid_pre"][0, -1, :].float() - cache_good[f"blocks.{layer}.hook_resid_pre"][0, -1, :].float()
                    diffs.append(d.cpu().numpy())
            
            pca = PCA(n_components=k)
            pca.fit(np.stack(diffs))
            subspaces[layer] = torch.tensor(pca.components_, dtype=torch.float32, device="cuda")
        return subspaces

    # 2. Projection Hook ì •ì˜
    def make_robust_projection_hook(vectors, coeff):
        def hook_fn(resid_pre, hook):
            h_fp32 = resid_pre.float()
            dot_products = torch.einsum("bsd,kd->bsk", h_fp32, vectors)
            h_parallel = torch.einsum("bsk,kd->bsd", dot_products, vectors)
            h_prime = h_fp32 - coeff * h_parallel
            return h_prime.to(resid_pre.dtype)
        return hook_fn

    # ìˆ˜ìˆ  ì§‘ë„
    refusal_subspaces = get_optimized_subspaces(k, target_layers)
    layer_hooks = [(f"blocks.{l}.hook_resid_pre", make_robust_projection_hook(refusal_subspaces[l], coeff)) for l in target_layers]

    gc.collect()
    torch.cuda.empty_cache()

    # ê²°ê³¼ ìƒì„±
    test_prompt = apply_template(test_query)
    with model.hooks(fwd_hooks=layer_hooks):
        output = model.generate(test_prompt, max_new_tokens=512, temperature=0.7, top_p=0.9)
    
    return output.replace(test_prompt, "")
```
